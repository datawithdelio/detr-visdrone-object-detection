{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuajjTSPlvYH"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "# Training DETR on Custom Dataset\n",
        "Tutorial Video [link](https://www.youtube.com/watch?v=RkhXoj_Vvr4&lc=UgwHlStd7pa4KMszFQx4AaABAg&ab_channel=DeepReader)\n",
        "Code based on [End-to-end Object Detection with Transformer](https://github.com/facebookresearch/detr)\n",
        "\n",
        "##Requirements\n",
        "\n",
        "- Make sure runtime is set to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "netVcyCeqPJL"
      },
      "source": [
        "##Step 1.\n",
        "\n",
        "Clone repo from github and check requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnDlkrt1CRwO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/Users/deliorincon/Desktop/Kumar\")\n",
        "\n",
        "\n",
        "# !rm -rf /content/detr_tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EVptt1TCCDH",
        "outputId": "961f78fa-5ae0-4e33-92fb-b793745189ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2UzHLLICFkv",
        "outputId": "bea2c46f-611e-4b11-9f82-f286965b1d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "2.18.0\n",
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "#Downgrade Tensorflow - you will need to restart or reload after running this - added by YK 1/13/2023\n",
        "%tensorflow_version 2.x\n",
        "from tensorboard import version; print(version.VERSION)\n",
        "import tensorflow as tf; print(tf.__version__)\n",
        "#%tensorboard_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FI4OqIc85AR",
        "outputId": "a7db9b43-16b5-428c-9fd9-d6a6c4a09bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "#check TensortFlow - added by YK 1/13/2023\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFFQO17Mqg4b",
        "outputId": "e05b764c-cb4e-4852-f738-eead8507f776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'detr_tutorial'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 199 (delta 45), reused 45 (delta 45), pack-reused 150 (from 1)\u001b[K\n",
            "Receiving objects: 100% (199/199), 134.27 KiB | 3.95 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# thedeepreader is outdated\n",
        "#!git clone https://github.com/thedeepreader/detr_tutorial\n",
        "\n",
        "# This repo should be up-to-date with FB's updates, if not then post an issue - Neil\n",
        "!git clone https://github.com/Neveon/detr_tutorial.git\n",
        "\n",
        "os.chdir('/content/detr_tutorial/detr/')\n",
        "\n",
        "!pip install -qr requirements.txt\n",
        "\n",
        "os.chdir('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFEXB7wQ0HTt"
      },
      "outputs": [],
      "source": [
        "\n",
        "#%ls -l '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/train/' | grep 'jpg' | wc -l\n",
        "#%ls -l '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/test/' | grep 'jpg' | wc -l\n",
        "#%ls -l '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/valid/' | grep 'jpg' | wc -l\n",
        "\n",
        "#Uko Original Data\n",
        "%ls -l '/content/drive/MyDrive/Uko Data/data/train' | grep 'jpg' | wc -l\n",
        "%ls -l '/content/drive/MyDrive/Uko Data/data/test' | grep 'jpg' | wc -l\n",
        "%ls -l '/content/drive/MyDrive/Uko Data/data/valid' | grep 'jpg' | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXIGP9XK6oYP"
      },
      "source": [
        "## Remove Corrupted JPG Images and their corresponding TXT files\n",
        "<b><u>NOTE</u>: Only needed to run if the datasets are updated with new images</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xab-ZuBx8DsF"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "import cv2 as cv\n",
        "\n",
        "bad_files = 0\n",
        "for f in ['train', 'valid', 'test']:\n",
        "  #data = '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/{}/'.format(f)\n",
        "\n",
        "  #Uko Original Data\n",
        "  data = '/content/drive/MyDrive/Uko Data/data/{}/'.format(f)\n",
        "  for filename in listdir(data):\n",
        "    if filename.endswith('.jpg'):\n",
        "      im = cv.imread(data + filename)\n",
        "      val = cv.haveImageReader(data + filename) # returns false if not readable by cv2\n",
        "      if(not val):\n",
        "        bad_files += 1\n",
        "        print(f + \"/\" + filename)\n",
        "        # remove files\n",
        "        os.remove(data+filename)\n",
        "        os.remove(data+filename.replace(\".jpg\", \".txt\"))\n",
        "print(\"Number of bad jpgs: \", bad_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UDpxg6IqU9b"
      },
      "source": [
        "## Step 2. Download Pretrained Weights & Convert Dataset to COCO Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nfCxCkM5pTw"
      },
      "source": [
        "##Download weights for transfer learning to speed up training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce-IR0pF5vQo"
      },
      "outputs": [],
      "source": [
        "!gdown https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtXNZccE0E1S"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/detr/detr-r101-dc5-a2e86def.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-qfHydnot-d"
      },
      "source": [
        "## **NOTE: Unless you either (1) Don't have COCO formatted data or (2) Updated your dataset, there is no need to re-run this function because it takes a long time to convert the data into COCO format.** Reuse COCO data to save time.\n",
        "\n",
        "Edit face.py based on number of classes in your dataset\n",
        "\n",
        "\n",
        "- Move to the dataset folder, and convert downloaded  dataset into COCO format\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fUibCJOqxli"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/dataset/face_to_coco.py\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import itertools\n",
        "import numpy as np\n",
        "import glob\n",
        "import scipy.io as sio\n",
        "from pycocotools import mask as cocomask\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "categories = []\n",
        "items = [\"supercategory\",\n",
        "        \"name\",\n",
        "        \"id\"]\n",
        "\n",
        "def makeCategories(fname):\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      name = l.strip()\n",
        "      temp = {items[0]:\"none\",items[1]:name,items[2]:i}\n",
        "      categories.append(temp)\n",
        "  return categories\n",
        "\n",
        "# Use classes.txt file here - this is used to label each object found in the image\n",
        "categories = makeCategories(\"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/classes.txt\")\n",
        "\n",
        "phases = [\"train\",\"valid\"]\n",
        "for phase in phases:\n",
        "    root_path = \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/{}/\".format(phase)\n",
        "    #gt_path = os.path.join(\"wider_face_split/wider_face_{}.mat\".format(phase))\n",
        "    json_file = root_path + \"{}.json\".format(phase)\n",
        "\n",
        "    #gt = sio.loadmat(gt_path)\n",
        "    #event_list = gt.get(\"event_list\")\n",
        "    #file_list = gt.get(\"file_list\")\n",
        "    #face_bbox_list = gt.get(\"face_bbx_list\")\n",
        "\n",
        "    res_file = {\n",
        "        \"categories\": categories,\n",
        "        \"images\": [],\n",
        "        \"annotations\": []\n",
        "    }\n",
        "\n",
        "    annot_count = 0\n",
        "    image_id = 0\n",
        "    processed = 0\n",
        "    print(\"Obtaining label and img names for {} phase. This may take a while..\".format(phase))\n",
        "    #Obtain label and img name for each phase\n",
        "    file_list = glob.glob(root_path + \"/*.txt\")\n",
        "    img_paths = []\n",
        "    filenames = []\n",
        "    for i,val in enumerate(file_list):\n",
        "        # # make sure files exist\n",
        "        # if os.path.exists(file_list[i].replace(\".txt\",\".jpg\")):\n",
        "        img_paths.append(file_list[i].replace(\".txt\",\".jpg\"))\n",
        "        filenames.append(file_list[i].replace(\".txt\",\".jpg\").replace(root_path,\"\"))\n",
        "        # else:\n",
        "        #   continue\n",
        "\n",
        "\n",
        "        #Image annotation\n",
        "        img = cv2.imread(img_paths[i])\n",
        "\n",
        "        #Added this line to get rid of NoneType error -AT\n",
        "        if img is not None:\n",
        "          img_h, img_w, channels = img.shape\n",
        "        img_elem = {\"file_name\": filenames[i],\n",
        "                    \"height\": img_h,\n",
        "                    \"width\": img_w,\n",
        "                    \"id\": image_id}\n",
        "\n",
        "        # print(img_elem)\n",
        "\n",
        "        res_file[\"images\"].append(img_elem)\n",
        "        with open(val,\"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                key = line.strip()\n",
        "                coords = key.split()\n",
        "\n",
        "                x_center = (float(coords[1])*(img_w))\n",
        "                y_center = (float(coords[2])*(img_h))\n",
        "                width = (float(coords[3])*img_w)\n",
        "                height = (float(coords[4])*img_h)\n",
        "                category_id = int(coords[0]) # label\n",
        "\n",
        "                mid_x = int(x_center-width/2)\n",
        "                mid_y = int(y_center-height/2)\n",
        "                width = int(width)\n",
        "                height = int(height)\n",
        "\n",
        "                area = width*height\n",
        "                poly = [[mid_x, mid_y],\n",
        "                        [width, height],\n",
        "                        [width, height],\n",
        "                        [mid_x, mid_y]]\n",
        "\n",
        "                annot_elem = {\n",
        "                    \"id\": annot_count,\n",
        "                    \"bbox\": [\n",
        "                        float(mid_x),\n",
        "                        float(mid_y),\n",
        "                        float(width),\n",
        "                        float(height)\n",
        "                    ],\n",
        "                    \"segmentation\": list([poly]),\n",
        "                    \"image_id\": image_id,\n",
        "                    \"ignore\": 0,\n",
        "                    \"category_id\": 0,\n",
        "                    \"iscrowd\": 0,\n",
        "                    \"area\": float(area)\n",
        "                }\n",
        "                res_file[\"annotations\"].append(annot_elem)\n",
        "                annot_count += 1\n",
        "\n",
        "            image_id += 1\n",
        "\n",
        "            processed += 1\n",
        "\n",
        "    with open(json_file, \"w\") as f:\n",
        "      json_str = json.dumps(res_file)\n",
        "      f.write(json_str)\n",
        "\n",
        "    print(\"Processed {} {} images...\".format(processed, phase))\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiJS5Uka7YjN"
      },
      "outputs": [],
      "source": [
        "# Upload JSON files to the drive if you haven't updated dataset. Otherwise run this but it may take a long while to generate JSON - Neil\n",
        "!python3 /content/detr_tutorial/dataset/face_to_coco.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpHs8ZtbvRvB"
      },
      "outputs": [],
      "source": [
        "# Check if JSON files exist\n",
        "#!ls '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/train' | grep json\n",
        "\n",
        "#Uko Original Data\n",
        "!ls '/content/drive/MyDrive/Uko Data/data/train' | grep json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "017C6fJTxbpY"
      },
      "outputs": [],
      "source": [
        "#Check contents of JSON file and make sure data is correct\n",
        "#!cat '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/test/test.json'\n",
        "\n",
        "#Uko Original Data\n",
        "!cat '/content/drive/MyDrive/Uko Data/data/test/test.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lyw9LxJ6cHl"
      },
      "source": [
        "##Additional info:\n",
        "http://yoda.kean.edu/~sergeach/pollen/Pollen.v1.coco.rar\n",
        "\n",
        "https://github.com/tzutalin/labelImg\n",
        "\n",
        "https://www.makesense.ai/\n",
        "\n",
        "https://github.com/plotly/dash-detr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm7NDZ21wype"
      },
      "source": [
        "##Step 4.\n",
        "\n",
        "Edit loader for cooc dataset in face.py\n",
        "\n",
        "- in build method change around paths to how you need for your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3do4v6sGtKtq"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/detr_tutorial/detr/')\n",
        "print(os.path.abspath(os.curdir))\n",
        "\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb4dgSUtwx1n"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/detr/datasets/face.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Face dataset which returns image_id for evaluation.\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "from pycocotools import mask as Face_mask\n",
        "\n",
        "import datasets.transforms as T\n",
        "\n",
        "class FaceDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
        "        # print(\"IMAGE FOLDER: \", img_folder)\n",
        "        # print(\"ANN_FILE: \", ann_file)\n",
        "        super(FaceDetection, self).__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "        self.prepare = ConvertFacePolysToMask(return_masks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super(FaceDetection, self).__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = {'image_id': image_id, 'annotations': target}\n",
        "        img, target = self.prepare(img, target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def convert_Face_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = Face_mask.frPyObjects(polygons, height, width)\n",
        "        mask = Face_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertFacePolysToMask(object):\n",
        "    def __init__(self, return_masks=False):\n",
        "        self.return_masks = return_masks\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "        image_id = torch.tensor([image_id])\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        if self.return_masks:\n",
        "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "            masks = convert_Face_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        if self.return_masks:\n",
        "            masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        if self.return_masks:\n",
        "            target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to Face api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
        "        target[\"area\"] = area[keep]\n",
        "        target[\"iscrowd\"] = iscrowd[keep]\n",
        "\n",
        "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
        "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def make_face_transforms(image_set):\n",
        "\n",
        "    normalize = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
        "\n",
        "    if image_set == 'train':\n",
        "        return T.Compose([\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.RandomSelect(\n",
        "                T.RandomResize(scales, max_size=1333),\n",
        "                T.Compose([\n",
        "                    T.RandomResize([400, 500, 600]),\n",
        "                    T.RandomSizeCrop(384, 600),\n",
        "                    T.RandomResize(scales, max_size=1333),\n",
        "                ])\n",
        "            ),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    if image_set == 'val':\n",
        "        return T.Compose([\n",
        "            T.RandomResize([800], max_size=1333),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    raise ValueError(f'unknown {image_set}')\n",
        "\n",
        "\n",
        "def build(image_set, args):\n",
        "    root = Path(args.data_path)\n",
        "    print(root / \"valid\", root / f'valid.json')\n",
        "    assert root.exists(), f'provided Face path {root} does not exist'\n",
        "    mode = 'instances'\n",
        "    PATHS = {\n",
        "        \"train\": (root / \"train/\", root / f'train/train.json'),\n",
        "        \"val\": (root / \"valid/\", root / f'valid/valid.json'),\n",
        "        \"test\": (root / \"test/\", root / f'test/test.json')\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    dataset = FaceDetection(img_folder, ann_file, transforms=make_face_transforms(image_set), return_masks=args.masks)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-VTY_rFGXrP"
      },
      "outputs": [],
      "source": [
        "# Make sure pwd is within detr/ directory\n",
        "!pwd\n",
        "%run /content/detr_tutorial/detr/datasets/face.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu86hn280RKM"
      },
      "source": [
        "##Step 5\n",
        "\n",
        "edit main.py\n",
        "\n",
        "- change query flag to max number of bboxs per image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48_-oVZG0V_7"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/detr/main.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "import argparse\n",
        "import datetime\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "\n",
        "import datasets\n",
        "import util.misc as utils\n",
        "from datasets import build_dataset, get_coco_api_from_dataset\n",
        "from engine import evaluate, train_one_epoch\n",
        "from models import build_model\n",
        "\n",
        "# print(build_matcher)\n",
        "\n",
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=2, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--lr_drop', default=200, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    # * Backbone\n",
        "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
        "                        help=\"Name of the convolutional backbone to use\")\n",
        "    parser.add_argument('--dilation', action='store_true',\n",
        "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
        "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
        "                        help=\"Type of positional embedding to use on top of the image features\")\n",
        "\n",
        "    # * Transformer\n",
        "    parser.add_argument('--enc_layers', default=6, type=int,\n",
        "                        help=\"Number of encoding layers in the transformer\")\n",
        "    parser.add_argument('--dec_layers', default=6, type=int,\n",
        "                        help=\"Number of decoding layers in the transformer\")\n",
        "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
        "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
        "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
        "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
        "    parser.add_argument('--dropout', default=0.1, type=float,\n",
        "                        help=\"Dropout applied in the transformer\")\n",
        "    parser.add_argument('--nheads', default=8, type=int,\n",
        "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
        "    parser.add_argument('--num_queries', default=30, type=int,\n",
        "                        help=\"Number of query slots\")\n",
        "    parser.add_argument('--pre_norm', action='store_true')\n",
        "\n",
        "    # * Segmentation\n",
        "    parser.add_argument('--masks', action='store_true',\n",
        "                        help=\"Train segmentation head if the flag is provided\")\n",
        "\n",
        "    # Loss\n",
        "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
        "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
        "    # * Matcher\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
        "                        help=\"L1 box coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
        "                        help=\"giou box coefficient in the matching cost\")\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
        "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
        "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
        "                        help=\"Relative classification weight of the no-object class\")\n",
        "\n",
        "    # dataset parameters\n",
        "    # parser.add_argument('--dataset_file', default='coco')\n",
        "    parser.add_argument('--dataset_file', default='face')\n",
        "    parser.add_argument('--data_path', type=str)\n",
        "    parser.add_argument('--coco_panoptic_path', type=str)\n",
        "    parser.add_argument('--remove_difficult', action='store_true')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--seed', default=42, type=int)\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "    parser.add_argument('--start_epoch', default=0, type=int, metavar='N',\n",
        "                        help='start epoch')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--num_workers', default=2, type=int)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--world_size', default=1, type=int,\n",
        "                        help='number of distributed processes')\n",
        "    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')\n",
        "    return parser\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    utils.init_distributed_mode(args)\n",
        "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        assert args.masks, \"Frozen training is meant for segmentation only\"\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    model, criterion, postprocessors = build_model(args)\n",
        "    model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
        "                                  weight_decay=args.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "\n",
        "    dataset_train = build_dataset(image_set='train', args=args)\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "\n",
        "    # print(\"Length of Dataset train: \", len(dataset_train))\n",
        "    # print(\"Length of Dataset val: \", len(dataset_val))\n",
        "    if args.distributed:\n",
        "        sampler_train = DistributedSampler(dataset_train)\n",
        "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, args.batch_size, drop_last=True)\n",
        "\n",
        "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                                   collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
        "                                  drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
        "\n",
        "    data_loader_val = DataLoader(dataset_val, 1, sampler=sampler_val,\n",
        "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
        "\n",
        "    if args.dataset_file == \"coco_panoptic\":\n",
        "        # We also evaluate AP during panoptic training, on original coco DS\n",
        "        coco_val = datasets.coco.build(\"val\", args)\n",
        "        base_ds = get_coco_api_from_dataset(coco_val)\n",
        "    else:\n",
        "        base_ds = get_coco_api_from_dataset(dataset_val)\n",
        "\n",
        "    if args.frozen_weights is not None:\n",
        "        checkpoint = torch.load(args.frozen_weights, map_location='cpu')\n",
        "        model_without_ddp.detr.load_state_dict(checkpoint['model'])\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    if args.resume:\n",
        "        if args.resume.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.resume, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "\n",
        "        del checkpoint[\"model\"][\"class_embed.weight\"]\n",
        "        del checkpoint[\"model\"][\"class_embed.bias\"]\n",
        "        del checkpoint[\"model\"][\"query_embed.weight\"]\n",
        "\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'], strict=False)\n",
        "        if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "            args.start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if args.eval:\n",
        "        test_stats, coco_evaluator = evaluate(model, criterion, postprocessors,\n",
        "                                              data_loader_val, base_ds, device, args.output_dir)\n",
        "        if args.output_dir:\n",
        "            utils.save_on_master(coco_evaluator.coco_eval[\"bbox\"].eval, output_dir / \"eval.pth\")\n",
        "        return\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        if args.distributed:\n",
        "            sampler_train.set_epoch(epoch)\n",
        "        train_stats = train_one_epoch(\n",
        "            model, criterion, data_loader_train, optimizer, device, epoch,\n",
        "            args.clip_max_norm)\n",
        "        lr_scheduler.step()\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
        "            # extra checkpoint before LR drop and every 100 epochs\n",
        "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
        "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                utils.save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "\n",
        "        test_stats, coco_evaluator = evaluate(\n",
        "            model, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir\n",
        "        )\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and utils.is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "            # for evaluation logs\n",
        "            if coco_evaluator is not None:\n",
        "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "                if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                    filenames = ['latest.pth']\n",
        "                    if epoch % 50 == 0:\n",
        "                        filenames.append(f'{epoch:03}.pth')\n",
        "                    for name in filenames:\n",
        "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                   output_dir / \"eval\" / name)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "    args = parser.parse_args()\n",
        "    if args.output_dir:\n",
        "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBP_ALzg1UfF"
      },
      "source": [
        "##Step 6.\n",
        "\n",
        "Edit detr.py\n",
        "\n",
        "- in build method change number of classes to your datasets number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUp70XRB1kss"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/detr/models/detr.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "DETR model and criterion classes.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "from util import box_ops\n",
        "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
        "                       accuracy, get_world_size, interpolate,\n",
        "                       is_dist_avail_and_initialized)\n",
        "\n",
        "from .backbone import build_backbone\n",
        "from .matcher import build_matcher\n",
        "from .segmentation import (DETRsegm, PostProcessPanoptic, PostProcessSegm,\n",
        "                           dice_loss, sigmoid_focal_loss)\n",
        "from .transformer import build_transformer\n",
        "\n",
        "\n",
        "class DETR(nn.Module):\n",
        "    \"\"\" This is the DETR module that performs object detection \"\"\"\n",
        "    def __init__(self, backbone, transformer, num_classes, num_queries, aux_loss=False):\n",
        "        \"\"\" Initializes the model.\n",
        "        Parameters:\n",
        "            backbone: torch module of the backbone to be used. See backbone.py\n",
        "            transformer: torch module of the transformer architecture. See transformer.py\n",
        "            num_classes: number of object classes\n",
        "            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "                         DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "        self.aux_loss = aux_loss\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        \"\"\" The forward expects a NestedTensor, which consists of:\n",
        "               - samples.tensor: batched images, of shape [batch_size x 3 x H x W]\n",
        "               - samples.mask: a binary mask of shape [batch_size x H x W], containing 1 on padded pixels\n",
        "            It returns a dict with the following elements:\n",
        "               - \"pred_logits\": the classification logits (including no-object) for all queries.\n",
        "                                Shape= [batch_size x num_queries x (num_classes + 1)]\n",
        "               - \"pred_boxes\": The normalized boxes coordinates for all queries, represented as\n",
        "                               (center_x, center_y, height, width). These values are normalized in [0, 1],\n",
        "                               relative to the size of each individual image (disregarding possible padding).\n",
        "                               See PostProcess for information on how to retrieve the unnormalized bounding box.\n",
        "               - \"aux_outputs\": Optional, only returned when auxilary losses are activated. It is a list of\n",
        "                                dictionnaries containing the two above keys for each decoder layer.\n",
        "        \"\"\"\n",
        "        if isinstance(samples, (list, torch.Tensor)):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_logits': outputs_class[-1], 'pred_boxes': outputs_coord[-1]}\n",
        "        if self.aux_loss:\n",
        "            out['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord)\n",
        "        return out\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _set_aux_loss(self, outputs_class, outputs_coord):\n",
        "        # this is a workaround to make torchscript happy, as torchscript\n",
        "        # doesn't support dictionary with non-homogeneous values, such\n",
        "        # as a dict having both a Tensor and a list.\n",
        "        return [{'pred_logits': a, 'pred_boxes': b}\n",
        "                for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes, log=True):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "\n",
        "        if log:\n",
        "            # TODO this should probably be a separate loss, not hacked in this one here\n",
        "            losses['class_error'] = 100 - accuracy(src_logits[idx], target_classes_o)[0]\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(box_ops.generalized_box_iou(\n",
        "            box_ops.box_cxcywh_to_xyxy(src_boxes),\n",
        "            box_ops.box_cxcywh_to_xyxy(target_boxes)))\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the masks: the focal loss and the dice loss.\n",
        "           targets dicts must contain the key \"masks\" containing a tensor of dim [nb_target_boxes, h, w]\n",
        "        \"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "        masks = [t[\"masks\"] for t in targets]\n",
        "        # TODO use valid to mask invalid areas due to padding in loss\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # upsample predictions to the target size\n",
        "        src_masks = interpolate(src_masks[:, None], size=target_masks.shape[-2:],\n",
        "                                mode=\"bilinear\", align_corners=False)\n",
        "        src_masks = src_masks[:, 0].flatten(1)\n",
        "\n",
        "        target_masks = target_masks.flatten(1)\n",
        "        target_masks = target_masks.view(src_masks.shape)\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_focal_loss(src_masks, target_masks, num_boxes),\n",
        "            \"loss_dice\": dice_loss(src_masks, target_masks, num_boxes),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "            'masks': self.loss_masks\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "        if is_dist_avail_and_initialized():\n",
        "            torch.distributed.all_reduce(num_boxes)\n",
        "        num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        # In case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n",
        "        if 'aux_outputs' in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs['aux_outputs']):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    if loss == 'masks':\n",
        "                        # Intermediate masks losses are too costly to compute, we ignore them.\n",
        "                        continue\n",
        "                    kwargs = {}\n",
        "                    if loss == 'labels':\n",
        "                        # Logging is enabled only for the last layer\n",
        "                        kwargs = {'log': False}\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes, **kwargs)\n",
        "                    l_dict = {k + f'_{i}': v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "class PostProcess(nn.Module):\n",
        "    \"\"\" This module converts the model's output into the format expected by the coco api\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, target_sizes):\n",
        "        \"\"\" Perform the computation\n",
        "        Parameters:\n",
        "            outputs: raw outputs of the model\n",
        "            target_sizes: tensor of dimension [batch_size x 2] containing the size of each images of the batch\n",
        "                          For evaluation, this must be the original image size (before any data augmentation)\n",
        "                          For visualization, this should be the image size after data augment, but before padding\n",
        "        \"\"\"\n",
        "        out_logits, out_bbox = outputs['pred_logits'], outputs['pred_boxes']\n",
        "\n",
        "        assert len(out_logits) == len(target_sizes)\n",
        "        assert target_sizes.shape[1] == 2\n",
        "\n",
        "        prob = F.softmax(out_logits, -1)\n",
        "        scores, labels = prob[..., :-1].max(-1)\n",
        "\n",
        "        # convert to [x0, y0, x1, y1] format\n",
        "        boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)\n",
        "        # and from relative [0, 1] to absolute [0, height] coordinates\n",
        "        img_h, img_w = target_sizes.unbind(1)\n",
        "        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n",
        "        boxes = boxes * scale_fct[:, None, :]\n",
        "\n",
        "        results = [{'scores': s, 'labels': l, 'boxes': b} for s, l, b in zip(scores, labels, boxes)]\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def build(args):\n",
        "    if args.dataset_file == \"face\":\n",
        "        # num_classes = 3\n",
        "        num_classes = 44\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    backbone = build_backbone(args)\n",
        "\n",
        "    transformer = build_transformer(args)\n",
        "\n",
        "    model = DETR(\n",
        "        backbone,\n",
        "        transformer,\n",
        "        num_classes=num_classes,\n",
        "        num_queries=args.num_queries,\n",
        "        aux_loss=args.aux_loss,\n",
        "    )\n",
        "    if args.masks:\n",
        "        model = DETRsegm(model, freeze_detr=(args.frozen_weights is not None))\n",
        "    matcher = build_matcher(args)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': args.bbox_loss_coef}\n",
        "    weight_dict['loss_giou'] = args.giou_loss_coef\n",
        "    if args.masks:\n",
        "        weight_dict[\"loss_mask\"] = args.mask_loss_coef\n",
        "        weight_dict[\"loss_dice\"] = args.dice_loss_coef\n",
        "    # TODO this is a hack\n",
        "    if args.aux_loss:\n",
        "        aux_weight_dict = {}\n",
        "        for i in range(args.dec_layers - 1):\n",
        "            aux_weight_dict.update({k + f'_{i}': v for k, v in weight_dict.items()})\n",
        "        weight_dict.update(aux_weight_dict)\n",
        "\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    if args.masks:\n",
        "        losses += [\"masks\"]\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,\n",
        "                             eos_coef=args.eos_coef, losses=losses)\n",
        "    criterion.to(device)\n",
        "    postprocessors = {'bbox': PostProcess()}\n",
        "    if args.masks:\n",
        "        postprocessors['segm'] = PostProcessSegm()\n",
        "        if args.dataset_file == \"coco_panoptic\":\n",
        "            is_thing_map = {i: i <= 90 for i in range(201)}\n",
        "            postprocessors[\"panoptic\"] = PostProcessPanoptic(is_thing_map, threshold=0.85)\n",
        "\n",
        "    return model, criterion, postprocessors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4IFtj6z2de3"
      },
      "source": [
        "## Step 7.\n",
        "Move to detr folder and run main.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV8TbkY34RCR"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/detr/util/misc.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "import pickle\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import Tensor\n",
        "\n",
        "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "\n",
        "    # serialized to a Tensor\n",
        "    buffer = pickle.dumps(data)\n",
        "    storage = torch.ByteStorage.from_buffer(buffer)\n",
        "    tensor = torch.ByteTensor(storage).to(\"cuda\")\n",
        "\n",
        "    # obtain Tensor size of each rank\n",
        "    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n",
        "    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n",
        "    dist.all_gather(size_list, local_size)\n",
        "    size_list = [int(size.item()) for size in size_list]\n",
        "    max_size = max(size_list)\n",
        "\n",
        "    # receiving Tensor from all ranks\n",
        "    # we pad the tensor because torch all_gather does not support\n",
        "    # gathering tensors of different shapes\n",
        "    tensor_list = []\n",
        "    for _ in size_list:\n",
        "        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n",
        "    if local_size != max_size:\n",
        "        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n",
        "        tensor = torch.cat((tensor, padding), dim=0)\n",
        "    dist.all_gather(tensor_list, tensor)\n",
        "\n",
        "    data_list = []\n",
        "    for size, tensor in zip(size_list, tensor_list):\n",
        "        buffer = tensor.cpu().numpy().tobytes()[:size]\n",
        "        data_list.append(pickle.loads(buffer))\n",
        "\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.no_grad():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}',\n",
        "                'max mem: {memory:.0f}'\n",
        "            ])\n",
        "        else:\n",
        "            log_msg = self.delimiter.join([\n",
        "                header,\n",
        "                '[{0' + space_fmt + '}/{1}]',\n",
        "                'eta: {eta}',\n",
        "                '{meters}',\n",
        "                'time: {time}',\n",
        "                'data: {data}'\n",
        "            ])\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def get_sha():\n",
        "    cwd = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "    def _run(command):\n",
        "        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n",
        "    sha = 'N/A'\n",
        "    diff = \"clean\"\n",
        "    branch = 'N/A'\n",
        "    try:\n",
        "        sha = _run(['git', 'rev-parse', 'HEAD'])\n",
        "        subprocess.check_output(['git', 'diff'], cwd=cwd)\n",
        "        diff = _run(['git', 'diff-index', 'HEAD'])\n",
        "        diff = \"has uncommited changes\" if diff else \"clean\"\n",
        "        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n",
        "    return message\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = list(zip(*batch))\n",
        "    batch[0] = nested_tensor_from_tensor_list(batch[0])\n",
        "    return tuple(batch)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list):\n",
        "    # type: (List[List[int]]) -> List[int]\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "class NestedTensor(object):\n",
        "    def __init__(self, tensors, mask: Optional[Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def to(self, device):\n",
        "        # type: (Device) -> NestedTensor # noqa\n",
        "        cast_tensor = self.tensors.to(device)\n",
        "        mask = self.mask\n",
        "        if mask is not None:\n",
        "            assert mask is not None\n",
        "            cast_mask = mask.to(device)\n",
        "        else:\n",
        "            cast_mask = None\n",
        "        return NestedTensor(cast_tensor, cast_mask)\n",
        "\n",
        "    def decompose(self):\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.tensors)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n",
        "    # TODO make this more general\n",
        "    if tensor_list[0].ndim == 3:\n",
        "        if torchvision._is_tracing():\n",
        "            # nested_tensor_from_tensor_list() does not export well to ONNX\n",
        "            # call _onnx_nested_tensor_from_tensor_list() instead\n",
        "            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n",
        "\n",
        "        # TODO make it support different-sized images\n",
        "        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n",
        "        batch_shape = [len(tensor_list)] + max_size\n",
        "        b, c, h, w = batch_shape\n",
        "        dtype = tensor_list[0].dtype\n",
        "        device = tensor_list[0].device\n",
        "        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "        for img, pad_img, m in zip(tensor_list, tensor, mask):\n",
        "            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "            m[: img.shape[1], :img.shape[2]] = False\n",
        "    else:\n",
        "        raise ValueError('not supported')\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "# _onnx_nested_tensor_from_tensor_list() is an implementation of\n",
        "# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n",
        "@torch.jit.unused\n",
        "def _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n",
        "    max_size = []\n",
        "    for i in range(tensor_list[0].dim()):\n",
        "        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n",
        "        max_size.append(max_size_i)\n",
        "    max_size = tuple(max_size)\n",
        "\n",
        "    # work around for\n",
        "    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
        "    # m[: img.shape[1], :img.shape[2]] = False\n",
        "    # which is not yet supported in onnx\n",
        "    padded_imgs = []\n",
        "    padded_masks = []\n",
        "    for img in tensor_list:\n",
        "        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n",
        "        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n",
        "        padded_imgs.append(padded_img)\n",
        "\n",
        "        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n",
        "        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n",
        "        padded_masks.append(padded_mask.to(torch.bool))\n",
        "\n",
        "    tensor = torch.stack(padded_imgs)\n",
        "    mask = torch.stack(padded_masks)\n",
        "\n",
        "    return NestedTensor(tensor, mask=mask)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    if target.numel() == 0:\n",
        "        return [torch.zeros([], device=output.device)]\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n",
        "    \"\"\"\n",
        "    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n",
        "    This will eventually be supported natively by PyTorch, and this\n",
        "    class can go away.\n",
        "    \"\"\"\n",
        "    if float(torchvision.__version__[:3]) < 0.7:\n",
        "        if input.numel() > 0:\n",
        "            return torch.nn.functional.interpolate(\n",
        "                input, size, scale_factor, mode, align_corners\n",
        "            )\n",
        "\n",
        "        output_shape = _output_size(2, input, size, scale_factor)\n",
        "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
        "        return _new_empty_tensor(input, output_shape)\n",
        "    else:\n",
        "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJ4HL21jeM8"
      },
      "outputs": [],
      "source": [
        "%run /content/detr_tutorial/detr/util/misc.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj7w6mfO783W"
      },
      "source": [
        "###Save weights to google drive so if colab crashes you dont lose all your data :)\n",
        "\n",
        "- send it there via the output_dir tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON3tEeMR2f4n"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/detr_tutorial/detr/')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWcHPab1qdX"
      },
      "source": [
        "Change the number of `--epochs` to train.\n",
        "\n",
        "<b>NOTE</b>: Epoch 0 corresponds to `--epochs 1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5YQlQ7z2x1t"
      },
      "outputs": [],
      "source": [
        "# num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n",
        "#                          DETR can detect in a single image. For COCO, we recommend 100 queries.\n",
        "#!python3 main.py --epochs 11 --dataset_file face --data_path '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_0' --resume /content/detr-r50-e632da11.pth\n",
        "\n",
        "#Uko Original Data\n",
        "!python3 main.py --epochs 11 --dataset_file face --data_path '/content/drive/MyDrive/Uko Data/data' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_0' --resume /content/detr-r50-e632da11.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bxYGmGT2LBu"
      },
      "outputs": [],
      "source": [
        "# !python3 main.py --dataset_file face --data_path /content/detr_tutorial/dataset/data/train --output_dir /content/drive/My\\ Drive/Colab\\ Notebooks/PollenImageClassification/FaceBookDETRWeights/TGW_6_8_21 --resume /content/detr-r101-dc5-a2e86def.pth --dilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ8gK-Zy-S4e"
      },
      "source": [
        "##Restart from saved weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVtRPaOG2qEQ"
      },
      "outputs": [],
      "source": [
        "#!python3 main.py --epochs 10 --data_path '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_9/' --resume '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_8/checkpoint.pth'\n",
        "\n",
        "#Uko Original Data\n",
        "!python3 main.py --epochs 10 --data_path '/content/drive/MyDrive/Uko Data/data' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_9/' --resume '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_8/checkpoint.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ7UgyAsuhW7"
      },
      "source": [
        "## Function for training and saving epochs automatically\n",
        "\n",
        "<b><u>NOTE</u>:</b> It is recommended you use Colab Pro+. From my experience, Colab Pro+ trained an epoch in 1/3 of the time it would take without a subscription.\n",
        "<br/><br/><br/>\n",
        "\n",
        "Set your current epoch as the epoch with latest checkpoint you have.\n",
        "\n",
        "Set your ending epoch.\n",
        "\n",
        "You can edit the range to determine how many steps you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vcxb3HduugcG"
      },
      "outputs": [],
      "source": [
        "current_checkpoint = 0\n",
        "final_epoch = 300\n",
        "\n",
        "num_queries = 50 # Change as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVFAcx1yvo7d"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "for epoch in range(current_checkpoint+1, final_epoch+1): # we are training current_checkpoint+1 (inclusive) to final_epoch+1 (exclusive)\n",
        "  print(\"Currently training epoch {}...\".format(epoch))\n",
        "\n",
        "  # Change directories/filenames as you wish\n",
        "  #command = [\"python3\", \"main.py\", \"--epochs\", \"{}\".format(current_checkpoint+2), \"--num_queries\",\"{}\".format(num_queries), \"--data_path\", \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/\", \"--output_dir\", \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_{}/\".format(current_checkpoint+1), \"--resume\", \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_{}/checkpoint.pth\".format(current_checkpoint)]\n",
        "  #Uko Original Data\n",
        "  command = [\"python3\", \"main.py\", \"--epochs\", \"{}\".format(current_checkpoint+2), \"--num_queries\",\"{}\".format(num_queries), \"--data_path\", \"/content/drive/MyDrive/Uko Data/data\", \"--output_dir\", \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_{}/\".format(current_checkpoint+1), \"--resume\", \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_{}/checkpoint.pth\".format(current_checkpoint)]\n",
        "  child = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True) # universal_newlines allow stream to be a text stream\n",
        "  out, err = child.communicate()\n",
        "  print(out)\n",
        "\n",
        "  current_checkpoint += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KefNPrys6tG3"
      },
      "source": [
        "## Step 8\n",
        "\n",
        "Run on test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka57Rtb50gWv"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/detr_tutorial/detr/test.py\n",
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
        "\"\"\"\n",
        "Train and eval functions used in main.py\n",
        "\"\"\"\n",
        "import math\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import torch\n",
        "\n",
        "import util.misc as utils\n",
        "\n",
        "from models import build_model\n",
        "from datasets.face import make_face_transforms\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h,\n",
        "                          img_w, img_h\n",
        "                          ], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def get_images(in_path):\n",
        "    img_files = []\n",
        "    for (dirpath, dirnames, filenames) in os.walk(in_path):\n",
        "        for file in filenames:\n",
        "            filename, ext = os.path.splitext(file)\n",
        "            ext = str.lower(ext)\n",
        "            if ext == '.jpg' or ext == '.jpeg' or ext == '.gif' or ext == '.png' or ext == '.pgm':\n",
        "                img_files.append(os.path.join(dirpath, file))\n",
        "\n",
        "    return img_files\n",
        "\n",
        "\n",
        "def getClasses(fname):\n",
        "  CLASSES = []\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      #print(l)\n",
        "      name = l.strip()\n",
        "      CLASSES.append(name)\n",
        "  return CLASSES\n",
        "\n",
        "\n",
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)\n",
        "    parser.add_argument('--lr', default=1e-4, type=float)\n",
        "    parser.add_argument('--lr_backbone', default=1e-5, type=float)\n",
        "    parser.add_argument('--batch_size', default=6, type=int)\n",
        "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
        "    parser.add_argument('--epochs', default=300, type=int)\n",
        "    parser.add_argument('--lr_drop', default=200, type=int)\n",
        "    parser.add_argument('--clip_max_norm', default=0.1, type=float,\n",
        "                        help='gradient clipping max norm')\n",
        "\n",
        "    # Model parameters\n",
        "    parser.add_argument('--frozen_weights', type=str, default=None,\n",
        "                        help=\"Path to the pretrained model. If set, only the mask head will be trained\")\n",
        "    # * Backbone\n",
        "    parser.add_argument('--backbone', default='resnet50', type=str,\n",
        "                        help=\"Name of the convolutional backbone to use\")\n",
        "    parser.add_argument('--dilation', action='store_true',\n",
        "                        help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
        "    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),\n",
        "                        help=\"Type of positional embedding to use on top of the image features\")\n",
        "\n",
        "    # * Transformer\n",
        "    parser.add_argument('--enc_layers', default=6, type=int,\n",
        "                        help=\"Number of encoding layers in the transformer\")\n",
        "    parser.add_argument('--dec_layers', default=6, type=int,\n",
        "                        help=\"Number of decoding layers in the transformer\")\n",
        "    parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
        "                        help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
        "    parser.add_argument('--hidden_dim', default=256, type=int,\n",
        "                        help=\"Size of the embeddings (dimension of the transformer)\")\n",
        "    parser.add_argument('--dropout', default=0.1, type=float,\n",
        "                        help=\"Dropout applied in the transformer\")\n",
        "    parser.add_argument('--nheads', default=8, type=int,\n",
        "                        help=\"Number of attention heads inside the transformer's attentions\")\n",
        "    parser.add_argument('--num_queries', default=10, type=int,\n",
        "                        help=\"Number of query slots\")\n",
        "    parser.add_argument('--pre_norm', action='store_true')\n",
        "\n",
        "    # * Segmentation\n",
        "    parser.add_argument('--masks', action='store_true',\n",
        "                        help=\"Train segmentation head if the flag is provided\")\n",
        "\n",
        "    # # Loss\n",
        "    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',\n",
        "                        help=\"Disables auxiliary decoding losses (loss at each layer)\")\n",
        "    # * Matcher\n",
        "    parser.add_argument('--set_cost_class', default=1, type=float,\n",
        "                        help=\"Class coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_bbox', default=5, type=float,\n",
        "                        help=\"L1 box coefficient in the matching cost\")\n",
        "    parser.add_argument('--set_cost_giou', default=2, type=float,\n",
        "                        help=\"giou box coefficient in the matching cost\")\n",
        "    # * Loss coefficients\n",
        "    parser.add_argument('--mask_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--dice_loss_coef', default=1, type=float)\n",
        "    parser.add_argument('--bbox_loss_coef', default=5, type=float)\n",
        "    parser.add_argument('--giou_loss_coef', default=2, type=float)\n",
        "    parser.add_argument('--eos_coef', default=0.1, type=float,\n",
        "                        help=\"Relative classification weight of the no-object class\")\n",
        "\n",
        "    # dataset parameters\n",
        "    parser.add_argument('--dataset_file', default='face')\n",
        "    parser.add_argument('--data_path', type=str)\n",
        "    parser.add_argument('--data_panoptic_path', type=str)\n",
        "    parser.add_argument('--remove_difficult', action='store_true')\n",
        "\n",
        "    parser.add_argument('--output_dir', default='',\n",
        "                        help='path where to save the results, empty for no saving')\n",
        "    parser.add_argument('--device', default='cuda',\n",
        "                        help='device to use for training / testing')\n",
        "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "\n",
        "    parser.add_argument('--thresh', default=0.5, type=float)\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer(images_path, model, postprocessors, device, output_path):\n",
        "    model.eval()\n",
        "    duration = 0\n",
        "    CLASSES = getClasses('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/classes.txt')\n",
        "    # CLASSES = ['pollen','grass','tree','weed']\n",
        "    print(CLASSES)\n",
        "\n",
        "    for img_sample in images_path:\n",
        "        filename = os.path.basename(img_sample)\n",
        "        print(\"processing...{}\".format(filename))\n",
        "        orig_image = Image.open(img_sample)\n",
        "        w, h = orig_image.size\n",
        "        transform = make_face_transforms(\"val\")\n",
        "        dummy_target = {\n",
        "            \"size\": torch.as_tensor([int(h), int(w)]),\n",
        "            \"orig_size\": torch.as_tensor([int(h), int(w)])\n",
        "        }\n",
        "        image, targets = transform(orig_image, dummy_target)\n",
        "        image = image.unsqueeze(0)\n",
        "        image = image.to(device)\n",
        "\n",
        "\n",
        "        conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n",
        "        hooks = [\n",
        "            model.backbone[-2].register_forward_hook(\n",
        "                        lambda self, input, output: conv_features.append(output)\n",
        "\n",
        "            ),\n",
        "            model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n",
        "                        lambda self, input, output: enc_attn_weights.append(output[1])\n",
        "\n",
        "            ),\n",
        "            model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n",
        "                        lambda self, input, output: dec_attn_weights.append(output[1])\n",
        "\n",
        "            ),\n",
        "\n",
        "        ]\n",
        "\n",
        "        start_t = time.perf_counter()\n",
        "        outputs = model(image)\n",
        "        end_t = time.perf_counter()\n",
        "\n",
        "        outputs[\"pred_logits\"] = outputs[\"pred_logits\"].cpu()\n",
        "        outputs[\"pred_boxes\"] = outputs[\"pred_boxes\"].cpu()\n",
        "\n",
        "        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "        # keep = probas.max(-1).values > 0.85\n",
        "        keep = probas.max(-1).values > args.thresh\n",
        "\n",
        "        bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], orig_image.size)\n",
        "        probas = probas[keep].cpu().data.numpy()\n",
        "\n",
        "        print(\"\\t\\tprobas:\")\n",
        "        print(probas)\n",
        "        print(\"\\n\\n\\n\")\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        conv_features = conv_features[0]\n",
        "        enc_attn_weights = enc_attn_weights[0]\n",
        "        dec_attn_weights = dec_attn_weights[0].cpu()\n",
        "\n",
        "        # get the feature map shape\n",
        "        h, w = conv_features['0'].tensors.shape[-2:]\n",
        "\n",
        "        if len(bboxes_scaled) == 0:\n",
        "            continue\n",
        "\n",
        "        img = np.array(orig_image)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        print(\"\\t\\tbboxes_scaled:\")\n",
        "        print(bboxes_scaled)\n",
        "        print(\"\\n\\n\\n\")\n",
        "        for idx, box in enumerate(bboxes_scaled):\n",
        "            bbox = box.cpu().data.numpy()\n",
        "            test = box.cpu().data.numpy().astype(np.int32)\n",
        "            bbox = bbox.astype(np.int32)\n",
        "            bbox = np.array([\n",
        "                [bbox[0], bbox[1]],\n",
        "                [bbox[2], bbox[1]],\n",
        "                [bbox[2], bbox[3]],\n",
        "                [bbox[0], bbox[3]],\n",
        "                ])\n",
        "            bbox = bbox.reshape((4, 2))\n",
        "            cv2.polylines(img, [bbox], True, (0, 255, 0), 2)\n",
        "\n",
        "            # probablity element at idx\n",
        "            p = probas[idx]\n",
        "            print(\"\\t\\tprobability of idx::\")\n",
        "            print(p)\n",
        "            print(\"\\n\\n\\n\")\n",
        "\n",
        "            # sort class probabilities\n",
        "            cl = probas.argsort()[-3:][::-1]\n",
        "\n",
        "            print(\"\\t\\tcl::\")\n",
        "            print(cl)\n",
        "            print(\"\\n\\n\\n\")\n",
        "\n",
        "            # text = CLASSES[cl[0]] +\" \"+ str(p[cl[0]])\n",
        "            # print(\"\\t\\ttext:\")\n",
        "            # print(text)\n",
        "            # print(\"\\n\\n\\n\")\n",
        "\n",
        "            # text += CLASSES[cl[1]] +\" \"+ str(p[cl[1]]) + CLASSES[cl[2]] +\" \"+ str(p[cl[2]])\n",
        "            # print(text)\n",
        "            # # fontScale\n",
        "            # fontScale = 1\n",
        "\n",
        "            # # Blue color in BGR\n",
        "            # color = (255, 0, 0)\n",
        "\n",
        "            # # Line thickness of 2 px\n",
        "            # thickness = 2\n",
        "            # font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            # sr = (test[0],test[1])\n",
        "            # cv2.putText(img,text,sr,font,1,color,1)\n",
        "\n",
        "        img_save_path = os.path.join(output_path, filename)\n",
        "        cv2.imwrite(img_save_path, img)\n",
        "\n",
        "        #imshow does not work in colab\n",
        "        #cv2_imshow(img)\n",
        "        #cv2.waitKey()\n",
        "\n",
        "        #plt.imshow(img)\n",
        "        #plt.show()\n",
        "        infer_time = end_t - start_t\n",
        "        duration += infer_time\n",
        "        print(\"Processing...{} ({:.3f}s)\".format(filename, infer_time))\n",
        "\n",
        "    avg_duration = duration / len(images_path)\n",
        "    print(\"Avg. Time: {:.3f}s\".format(avg_duration))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "    args = parser.parse_args()\n",
        "    if args.output_dir:\n",
        "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    model, _, postprocessors = build_model(args)\n",
        "    if args.resume:\n",
        "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        # To remove size mismatch error. Delete the query - Neil\n",
        "        del checkpoint[\"model\"][\"class_embed.weight\"]\n",
        "        del checkpoint[\"model\"][\"class_embed.bias\"]\n",
        "        del checkpoint[\"model\"][\"query_embed.weight\"]\n",
        "        # After size mismatch error, remove missing keys error by making Strict=False - Neil\n",
        "        model.load_state_dict(checkpoint['model'], strict=False)\n",
        "    model.to(device)\n",
        "    image_paths = get_images(args.data_path)\n",
        "\n",
        "    infer(image_paths, model, postprocessors, device, args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YihdKE3s1N45"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/detr_tutorial/detr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDmYDe5HDPBD"
      },
      "outputs": [],
      "source": [
        "# Remove output test images to rerun the test\n",
        "# !rm /content/*.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdd4H5Ur3ll0"
      },
      "source": [
        "The `--output_dir` is where the output test images would be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlUo7en5GZWs"
      },
      "outputs": [],
      "source": [
        "!python3 test.py --data_path '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/test' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50_TestResults/epoch_1/Thresh50' #--resume '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_1/checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lVb5c1B63Yv"
      },
      "outputs": [],
      "source": [
        "\n",
        "!python3 test.py --thresh 0.5 --data_path '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/test' --resume '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_1/checkpoint.pth' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50_TestResults/epoch_1/Thresh50'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNmN5x_Y-Gbg"
      },
      "source": [
        "this is added by YK 1/13/2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXcusCfm7jaB"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install qt5-default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teKsubXo72KA"
      },
      "source": [
        "test added by YK 1/13/2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xvtqsfM7kj8"
      },
      "outputs": [],
      "source": [
        "!python3 test.py --thresh 0.5 --data_path '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/data/test' --output_dir '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50_TestResults/epoch300/Thresh50' #--resume '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries50/epoch_300/checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmfXGXgHI0MS"
      },
      "outputs": [],
      "source": [
        "# Count number of images with objects detected at above threshold probability #was 88\n",
        "!ls /content | grep 'jpg'| wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I6jLqdUs192"
      },
      "source": [
        "## Visualizing Biases - Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYzXZxXjcYNR"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "#model_0 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_0/checkpoint.pth')\n",
        "model_0 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_0/checkpoint.pth')\n",
        "# print(model_0.items())\n",
        "# model_0.eval()\n",
        "# model_0 = model_0.cuda()\n",
        "\n",
        "#model_1 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_1/checkpoint.pth')\n",
        "model_1 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_1/checkpoint.pth')\n",
        "# model_1.eval()\n",
        "# model_1 = model_1.cuda()\n",
        "\n",
        "model_2 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_2/checkpoint.pth')\n",
        "# model_2.eval()\n",
        "# model_2 = model_2.cuda()\n",
        "\n",
        "#model_5 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_5/checkpoint.pth')\n",
        "model_5 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_5/checkpoint.pth')\n",
        "\n",
        "#model_8 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_8/checkpoint.pth')\n",
        "model_8 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_8/checkpoint.pth')\n",
        "\n",
        "#model_10 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_10/checkpoint.pth')\n",
        "model_10 = th.load('/content/drive/MyDrive/ColabNotebooks/PollenImageClassification/classNum44_numQueries50/epoch_10/checkpoint.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiCzoZ75DzdS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import seaborn as sb  # for visualization\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# print(model_0['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())  # bias for specific node from first transformer encoder layer\n",
        "m0t1 = model_0['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m0 = model_0['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()     # All biases for this layer\n",
        "\n",
        "# print(model_1['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())\n",
        "m1t1 = model_1['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m1 = model_1['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "# print(model_2['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())\n",
        "m2t1 = model_2['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m2 = model_2['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "# print(model_5['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())\n",
        "m5t1 = model_5['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m5 = model_5['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "# print(model_8['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())\n",
        "m8t1 = model_8['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m8 = model_8['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "# print(model_10['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item())\n",
        "m10t1 = model_10['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][0].item()\n",
        "m10 = model_10['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "# columns correspond to epoch number\n",
        "# row data is the bias value - number of the row is the node index in the layer\n",
        "\n",
        "# df = pd.DataFrame([[m0t1, m1t1, m2t1, m5t1, m8t1, m10t1]], columns=[0, 1, 2, 5, 8, 10])\n",
        "# df = pd.DataFrame([[m0, m1, m2, m5, m8, m10]], columns=[0, 1, 2, 5, 8, 10])\n",
        "\n",
        "df0 = pd.DataFrame(m0, columns=[0])\n",
        "df1 = pd.DataFrame(m1, columns=[1])\n",
        "df2 = pd.DataFrame(m2, columns=[2])\n",
        "df5 = pd.DataFrame(m5, columns=[5])\n",
        "df8 = pd.DataFrame(m8, columns=[8])\n",
        "df10 = pd.DataFrame(m10, columns=[10])\n",
        "\n",
        "df = pd.concat([df0, df1, df2, df5, df8, df10], axis=1) # concat along column axis\n",
        "# df\n",
        "# df.columns\n",
        "# df[:][:].values\n",
        "# df[:].values[:2] # df[col num here].values[bias index goes here]\n",
        "\n",
        "# sb.lineplot(x=df.columns, y=df[:].values[:]) # x axis is Number of Epochs and y axis is Bias\n",
        "lineplot = sb.lineplot(data=df[:][:10].transpose(), legend=\"full\")\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Bias Value\")\n",
        "lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRcJPIj5D4-D"
      },
      "outputs": [],
      "source": [
        "sb.scatterplot(x=[0, 1, 2, 5, 8, 10], y=[m0t1, m1t1, m2t1, m5t1, m8t1, m10t1]) # x axis is Number of Epochs and y axis is Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2XfNuukuTbD"
      },
      "outputs": [],
      "source": [
        "#print(model_10.keys())\n",
        "# print(model_10['model'].keys())\n",
        "model_keys = model_10['model'].keys()\n",
        "count_layers = 0\n",
        "for key in model_keys:\n",
        "  count_layers += 1\n",
        "  # if ('transformer' in key and 'weight' in key) or ('transformer' in key and 'bias' in key):\n",
        "  # if ('transformer' in key and 'bias' in key):\n",
        "  if ('transformer' in key and 'weight' in key):\n",
        "    print(\"\\n\\n{}\\n\\n\".format(key))\n",
        "    # break\n",
        "    # print(model_10['model'][key])\n",
        "    count_layers += 1\n",
        "\n",
        "print(\"Number of transformer layers: {}\".format(count_layers))\n",
        "# print('transformer.encoder.layers.0.self_attn.in_proj_bias')\n",
        "# print(model_10['model']['transformer.encoder.layers.0.self_attn.in_proj_bias']) # bias for specific layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H405W0JlEhxE"
      },
      "source": [
        "## Building the DataFrame & Visualization\n",
        "**(WARNING: Building DataFrame can take a while, about ~28m on Colab Pro+)**\n",
        "\n",
        "Load a saved `hdf` file instead of building another DataFrame if you can"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q3b9TfaKuAD"
      },
      "source": [
        "## Building Bias DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbQp7PgqD8If"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import seaborn as sb    # for visualization\n",
        "import torch as th\n",
        "import pandas as pd     # To save biases in DataFrame\n",
        "\n",
        "final_epoch = 300\n",
        "\n",
        "# init dataframe to hold all bias values\n",
        "epoch_0 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_0/checkpoint.pth')\n",
        "bias_0 = epoch_0['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "bias_0.shape\n",
        "# df = pd.DataFrame(bias_0, columns=[0])\n",
        "\n",
        "for current_epoch in range(1,final_epoch+1): # end is exclusive\n",
        "  checkpointFile = '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_{}/checkpoint.pth'.format(current_epoch)\n",
        "  model = th.load(checkpointFile)\n",
        "  biases = model['model']['transformer.encoder.layers.0.self_attn.in_proj_bias'][:].cpu().numpy()\n",
        "\n",
        "  temp_df = pd.DataFrame(biases, columns=[current_epoch])\n",
        "\n",
        "  df = pd.concat([df, temp_df], axis=1) # concat along column axis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpyUCqeYBDr6"
      },
      "outputs": [],
      "source": [
        "df[:][:1].transpose() # transpose for visual plot - column is node index and row is epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0-7b-uD9M4"
      },
      "source": [
        "## Visualize Bias Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT5_o2FEE6lR"
      },
      "source": [
        "#### **OPTIONAL:** Load previously saved bias, if you have it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omIuu_i0E-Gv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn_biases.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM6CSjK8E-1d"
      },
      "source": [
        "#### Visualize the bias values converging over a number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMvrhoxoY3S-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "lineplot = sb.lineplot(data=df[:][:21].transpose(), legend=\"full\")  # df[:][:final node index here]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "# Add vertical line to visualize the approximate area where convergence occurs\n",
        "plt.axvline(x=200, alpha = 0.5, linestyle=\"--\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Bias Value\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - Bias per Index Over 300 Epochs\")\n",
        "lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFtrU5EWYsJ6"
      },
      "outputs": [],
      "source": [
        "# All lines in one plot\n",
        "lineplot = sb.lineplot(data=df[:][:].transpose(), legend=False)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Bias Value\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - Bias per Index over 300 Epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pg9iv_RFUzR"
      },
      "source": [
        "## Saving Biases\n",
        "\n",
        "It can take a while to save large DataFrames as csv file, so we will use `.to_hdf(r'path/file.h5', key='stage', mode='w')` to significantly speed up saving data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUUuIE5qFoG3"
      },
      "outputs": [],
      "source": [
        "# Saving biases for transformer.encoder.layers.0.self_attn\n",
        "df.to_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn_biases.h5', key='stage', mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fng3YRvlKz0g"
      },
      "source": [
        "## Building Weight DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMr0AIa7jQHU"
      },
      "source": [
        "There are 786 nodes and 256 weights per node.\n",
        "\n",
        "### Build Weight DataFrame for **First** Weight of Each Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCMHhNbXK3AG"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import seaborn as sb    # for visualization\n",
        "import torch as th\n",
        "import pandas as pd     # To save weights in DataFrame\n",
        "\n",
        "final_epoch = 300\n",
        "\n",
        "# init dataframe to hold all weight values for epoch 0\n",
        "epoch_0 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_0/checkpoint.pth')\n",
        "weight_0 = epoch_0['model']['transformer.encoder.layers.0.self_attn.in_proj_weight'][:].cpu().numpy().reshape(256,768)[0]\n",
        "# weight_0.reshape(256,768)[0]\n",
        "weight_df = pd.DataFrame(weight_0, columns=[0])\n",
        "\n",
        "for current_epoch in range(1,final_epoch+1): # end is exclusive\n",
        "  checkpointFile = '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_{}/checkpoint.pth'.format(current_epoch)\n",
        "  model = th.load(checkpointFile)\n",
        "  weights = model['model']['transformer.encoder.layers.0.self_attn.in_proj_weight'][:].cpu().numpy().reshape(256,768)[0]\n",
        "\n",
        "  temp_df = pd.DataFrame(weights, columns=[current_epoch])\n",
        "\n",
        "  weight_df = pd.concat([weight_df, temp_df], axis=1) # concat along column axis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnnOA4NploHA"
      },
      "source": [
        "There are 786 nodes and 256 weights per node.\n",
        "\n",
        "### Build Weight DataFrame for **Second** Weight of Each Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfy5huX0lrUx"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import seaborn as sb    # for visualization\n",
        "import torch as th\n",
        "import pandas as pd     # To save weights in DataFrame\n",
        "\n",
        "final_epoch = 300\n",
        "\n",
        "# init dataframe to hold all weight values for epoch 0\n",
        "epoch_0 = th.load('/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_0/checkpoint.pth')\n",
        "weight_0 = epoch_0['model']['transformer.encoder.layers.0.self_attn.in_proj_weight'][:].cpu().numpy().reshape(256,768)[1]\n",
        "# weight_0.reshape(256,768)[0]\n",
        "weight1_df = pd.DataFrame(weight_0, columns=[0])\n",
        "\n",
        "for current_epoch in range(1,final_epoch+1): # end is exclusive\n",
        "  checkpointFile = '/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/allEpochs/epoch_{}/checkpoint.pth'.format(current_epoch)\n",
        "  model = th.load(checkpointFile)\n",
        "  weights = model['model']['transformer.encoder.layers.0.self_attn.in_proj_weight'][:].cpu().numpy().reshape(256,768)[1]\n",
        "\n",
        "  temp_df = pd.DataFrame(weights, columns=[current_epoch])\n",
        "\n",
        "  weight1_df = pd.concat([weight1_df, temp_df], axis=1) # concat along column axis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kniqfqozNOaH"
      },
      "source": [
        "## Saving Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MzmTnTfNZYT"
      },
      "outputs": [],
      "source": [
        "weight_df.to_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn.in_proj_weights_0.h5', key='stage', mode='w')\n",
        "weight1_df.to_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn.in_proj_weights_1.h5', key='stage', mode='w')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvSb9cnDMNDO"
      },
      "source": [
        "## Visualize Weight Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alHxICUUMUER"
      },
      "source": [
        "#### **OPTIONAL:** Load previous saved weights, if you have it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hNsbNmqMY4o"
      },
      "outputs": [],
      "source": [
        "weight0_df = pd.read_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn.in_proj_weights_0.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c7oqn0p2nR-"
      },
      "outputs": [],
      "source": [
        "weight1_df = pd.read_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn.in_proj_weights_1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGhsnAvbNgcQ"
      },
      "source": [
        "#### Visualize the weights\n",
        "\n",
        "**First** weight over 786 nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he3J3pMXMh_h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lineplot = sb.lineplot(data=weight0_df[:][:21].transpose(), legend=\"full\")  # weight_df[:][:final node index here]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "# Add vertical line to visualize the approximate area where convergence occurs\n",
        "plt.axvline(x=200, alpha = 0.5, linestyle=\"--\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Weight Value\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - First Weight per Index over 300 Epochs\")\n",
        "lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iQrbU0SYRif"
      },
      "outputs": [],
      "source": [
        "# All lines in one plot\n",
        "lineplot = sb.lineplot(data=weight0_df[:][:].transpose(), legend=False)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Weight Value\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - First Weight per Index over 300 Epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PIqCDI7203d"
      },
      "source": [
        "#### Visualize the weights\n",
        "\n",
        "**Second** weight over 786 nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poSjT_tJ203f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lineplot = sb.lineplot(data=weight1_df[:][:21].transpose(), legend=\"full\")  # weight_df[:][:final node index here]\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "# Add vertical line to visualize the approximate area where convergence occurs\n",
        "plt.axvline(x=200, alpha = 0.5, linestyle=\"--\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Weight Value\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - Second Weight per Index over 300 Epochs\")\n",
        "lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLaBRLDTbdS2"
      },
      "source": [
        "## Experimental: Plot the Derivative of the Biases per epoch\n",
        "\n",
        "To see the rate of change over the epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOO3961wl4wC"
      },
      "outputs": [],
      "source": [
        "biasCol0 = df.to_numpy().reshape(301, 768)[:,100]\n",
        "biasCol0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuS5nznfjFIU"
      },
      "outputs": [],
      "source": [
        "poly = numpy.poly1d(biasCol0)\n",
        "derivative = poly.deriv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cT5itjfjwRF"
      },
      "outputs": [],
      "source": [
        "print(biasCol0[0],\n",
        "derivative(biasCol0[0])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_zmAaBvlbc-"
      },
      "outputs": [],
      "source": [
        "f = lambda x: derivative(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ZnmrNKj-KB"
      },
      "outputs": [],
      "source": [
        "# sb.lineplot(data=df[:][:].transpose(), legend=False)\n",
        "sb.lineplot(data=f(biasCol0)[:][:], legend=False)\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Bias Rate of Change\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - Bias per Index over 300 Epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC_X0NSZDCbZ"
      },
      "source": [
        "## Experimental: Plot the Derivative of the Weights per epoch\n",
        "\n",
        "To see the rate of change over the epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hfg_TV1GDM2I"
      },
      "outputs": [],
      "source": [
        "weightCol0 = weight_df.to_numpy().reshape(301, 768)[:,100]\n",
        "weightCol0.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft-i5QTHDkUj"
      },
      "outputs": [],
      "source": [
        "poly_weight = numpy.poly1d(biasCol0)\n",
        "derivative_weight = poly.deriv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkAiccthDm9j"
      },
      "outputs": [],
      "source": [
        "f = lambda x: derivative_weight(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl2i3AoqDqhi"
      },
      "outputs": [],
      "source": [
        "sb.lineplot(data=f(weightCol0)[:][:], legend=False)\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Weight Rate of Change\")\n",
        "plt.title(\"Transformer Encoder Layer 0 - Weight per Index over 300 Epochs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJB5jHObGamw"
      },
      "source": [
        "# Visualizing Train/Test Results (DETR R50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPUCxeQiGssU"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\"train_loss\"\n",
        "\"train_loss_ce\"\n",
        "\"train_loss_bbox\"\n",
        "\"train_loss_giou\"\n",
        "\n",
        "\"test_loss\"\n",
        "\"test_loss_ce\"\n",
        "\"test_loss_bbox\"\n",
        "\"test_loss_giou\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbajk6kjJ7ai"
      },
      "outputs": [],
      "source": [
        "# Never grow a dataframe - https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\n",
        "\n",
        "# place values in dict and build dataframe with it\n",
        "results = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_loss_ce\": [],\n",
        "    \"train_loss_bbox\": [],\n",
        "    \"train_loss_giou\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_loss_ce\": [],\n",
        "    \"test_loss_bbox\": [],\n",
        "    \"test_loss_giou\": []\n",
        "}\n",
        "\n",
        "# Iterate through epochs and place values in dict\n",
        "for num in range(0, 301):\n",
        "  print(\"Adding results for epoch: {}...\".format(num))\n",
        "\n",
        "  resultsDirectory = \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/classNum44_numQueries100/epoch_{}/log.txt\".format(num)\n",
        "  file = open(resultsDirectory, \"r\")\n",
        "\n",
        "  contents = file.read()\n",
        "  log = ast.literal_eval(contents)\n",
        "\n",
        "  file.close()\n",
        "\n",
        "  # Verify it is type dictionary\n",
        "  # print(type(log))\n",
        "\n",
        "  for key in results.keys():\n",
        "    results[key].append(log[key])\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgVf-CEqNvFf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame.from_dict(results, orient='index').transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6Z_h4MBOVn5"
      },
      "outputs": [],
      "source": [
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys7m88zdN5GT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lineplot = sb.lineplot(data=df_results, legend=\"full\")\n",
        "\n",
        "plt.yticks([_ for _ in range(0,11)])\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "\n",
        "# Add vertical line to visualize the approximate area where convergence occurs\n",
        "plt.axvline(x=200, alpha = 0.5, linestyle=\"--\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Result Values\")\n",
        "plt.title(\"Train and Test Loss Results for Model with DETR R50 Pretrained Weights\")\n",
        "# lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdHFJnyWiY3_"
      },
      "source": [
        "# Visualizing Train/Test Results (DETR R101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BtHOfdBiY4B"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\"train_loss\"\n",
        "\"train_loss_ce\"\n",
        "\"train_loss_bbox\"\n",
        "\"train_loss_giou\"\n",
        "\n",
        "\"test_loss\"\n",
        "\"test_loss_ce\"\n",
        "\"test_loss_bbox\"\n",
        "\"test_loss_giou\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0MGT6j9iY4D"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "# Never grow a dataframe - https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it\n",
        "\n",
        "# place values in dict and build dataframe with it\n",
        "results = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_loss_ce\": [],\n",
        "    \"train_loss_bbox\": [],\n",
        "    \"train_loss_giou\": [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_loss_ce\": [],\n",
        "    \"test_loss_bbox\": [],\n",
        "    \"test_loss_giou\": []\n",
        "}\n",
        "\n",
        "# Iterate through epochs and place values in dict\n",
        "for num in range(0, 301):\n",
        "  print(\"Adding results for epoch: {}...\".format(num))\n",
        "\n",
        "  resultsDirectory = \"/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/detr_r101/classNum44_numQueries100/epoch_{}/log.txt\".format(num)\n",
        "  file = open(resultsDirectory, \"r\")\n",
        "\n",
        "  contents = file.read()\n",
        "  log = ast.literal_eval(contents)\n",
        "\n",
        "  file.close()\n",
        "\n",
        "  # Verify it is type dictionary\n",
        "  # print(type(log))\n",
        "\n",
        "  for key in results.keys():\n",
        "    results[key].append(log[key])\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITewCZsbiY4E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame.from_dict(results, orient='index').transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM4GI4gPiY4F"
      },
      "outputs": [],
      "source": [
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOs2j_DmiY4G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "lineplot = sb.lineplot(data=df_results, legend=\"full\")\n",
        "\n",
        "plt.yticks([_ for _ in range(0,11)])\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 12]\n",
        "\n",
        "# Add vertical line to visualize the approximate area where convergence occurs\n",
        "plt.axvline(x=200, alpha = 0.5, linestyle=\"--\", color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Number of Epochs\")\n",
        "plt.ylabel(\"Result Values\")\n",
        "plt.title(\"Train and Test Loss Results for Model with DETR R101 Pretrained Weights\")\n",
        "# lineplot.legend(title=\"Node Index\", bbox_to_anchor= (1.03, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ8FKvB6swzv"
      },
      "source": [
        "# Bias - Data Analysis\n",
        "\n",
        "We will use pandas and import h5 data for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRauipGizVLz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_hdf(r'/content/drive/MyDrive/Colab Notebooks/PollenImageClassification/transformer.encoder.layers.0.self_attn_biases.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLWr0AzFzl6a"
      },
      "outputs": [],
      "source": [
        "# each col is a node (768 nodes)\n",
        "# each row is an epoch (301 epochs)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYPzDl3a0aws"
      },
      "outputs": [],
      "source": [
        "df[:][300] # [node/bias][epoch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp-8OpID1xZi"
      },
      "outputs": [],
      "source": [
        "# Describe 768 biases of the 301st epoch\n",
        "df[:][300].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkbWUZzv2ZHN"
      },
      "source": [
        "### Bias Normal Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A15Tdzjb48E5"
      },
      "outputs": [],
      "source": [
        "df[:][300].plot.kde()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cAtMjYP_gNt"
      },
      "outputs": [],
      "source": [
        "import seaborn as sb\n",
        "sb.histplot(df[:][300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlXwgytecN8w"
      },
      "outputs": [],
      "source": [
        "df[:][16].plot.kde(title=\"Probability Density Function(PDF)  using Kernel Density Estimation(KDE) - Bandwidth method:scott\");\n",
        "df[:][16].plot.kde(bw_method=0.3, title=\"PDF using Kernel Density Estimation - Bandwidth value=0.3\");\n",
        "df[:][16].plot.kde(bw_method=3, title=\"PDF using Kernel Density Estimation - Bandwidth value=3\");\n",
        "handles = [mpatches.Patch(facecolor=plt.cm.Reds(100), label=\"bw_method=0.3\"),\n",
        "           mpatches.Patch(facecolor=plt.cm.Blues(100), label=\"bw_method=3\"),\n",
        "           mpatches.Patch(facecolor=plt.cm.Greens(100), label=\"Bandwidth method:scott\")]\n",
        "plt.legend(handles=handles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWp9xlhOOdYk"
      },
      "outputs": [],
      "source": [
        "sb.displot(df[:][300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdG1YobW_tMd"
      },
      "outputs": [],
      "source": [
        "df300 = pd.DataFrame(data= df[:][300])\n",
        "\n",
        "# quantile = df300.quantile([0.0, 0.9])\n",
        "\n",
        "df300quant05 = df300 <= df300.quantile(0.05)\n",
        "print(f\"There are {len(df300[df300quant05].dropna())} biases at or below the 5 percentile\")\n",
        "\n",
        "# df300[df300quant05].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRdkdl10cxsl"
      },
      "outputs": [],
      "source": [
        "df300quant95 = df300 >= df300.quantile(0.95)\n",
        "\n",
        "print(f\"There are {len(df300[df300quant95].dropna())} biases at or above the 95 percentile\")\n",
        "\n",
        "# df300[df300quant95].dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP_g_7qpbAH0"
      },
      "source": [
        "# Note to PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o3MvKhvbCj9"
      },
      "outputs": [],
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pnBzTaWbfW3",
        "outputId": "3b958fb8-7ae9-4376-903f-946878995799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eimnfy6qbJJ9"
      },
      "outputs": [],
      "source": [
        "!pip install pypandoc"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UXIGP9XK6oYP",
        "3I6jLqdUs192",
        "8Q3b9TfaKuAD",
        "4pg9iv_RFUzR",
        "XnnOA4NploHA",
        "kniqfqozNOaH",
        "nLaBRLDTbdS2"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kumar (3.12.4)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
